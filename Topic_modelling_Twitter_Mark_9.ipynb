{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T17:36:27.572444",
     "start_time": "2018-12-01T17:36:27.538755"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import scipy as sp;\n",
    "import sklearn;\n",
    "import sys;\n",
    "from nltk.corpus import stopwords;\n",
    "import nltk;\n",
    "from gensim.models import ldamodel\n",
    "import gensim.corpora;\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer;\n",
    "from sklearn.decomposition import NMF;\n",
    "from sklearn.preprocessing import normalize;\n",
    "import pickle;\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import subprocess\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "import re  # regular expressions (for playing with the text)\n",
    "from gensim.parsing.preprocessing import STOPWORDS # common english \"stop words\" -- a, the, etc.\n",
    "from gensim.parsing import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import nltk.data\n",
    "\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T17:18:40.020088",
     "start_time": "2018-12-01T17:18:38.971259"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions that needed to clean the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T17:18:42.201310",
     "start_time": "2018-12-01T17:18:42.167707"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_nonascii(text):\n",
    "    return ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "\n",
    "def separate_hashtags(text):\n",
    "    return set(part[1:] for part in text.split() if part.startswith(\"#\"))\n",
    "        \n",
    "def replace_abbrevs(text):\n",
    "    return re.sub(r'([a-zA-Z])([\\'\\-\\.])(?=[a-zA-Z])', r'\\1', text)\n",
    "\n",
    "def replace_email(text):\n",
    "    return re.sub(r'[\\w\\.\\-]+@[\\w\\.\\-]+', 'emailtoken', text)\n",
    "\n",
    "def replace_urls(text):\n",
    "    \"\"\"\n",
    "    Thank you https://www.pythonsheets.com/notes/python-rexp.html :-)\n",
    "    \"\"\"\n",
    "    \n",
    "    url_exp1 = re.compile(r'''\\b(https?:\\/\\/)? # match http or https\n",
    "    ([\\da-z\\.-]+)            # match domain\n",
    "    \\.([a-z\\.]{2,6})         # match domain (must be something.something)\n",
    "    ([\\/\\w\\.\\-=\\?]*)\\/?\\b    # match api or file or parameters\n",
    "    ''', re.X)\n",
    "    url_exp2 = re.compile(r'''\\b(https?:\\/\\/) # match http or https (must be there)\n",
    "    ([\\da-z\\.-]+)            # match domain\n",
    "    \\.?([a-z\\.]{2,6})        # match domain (this time may not have the \".something\")\n",
    "    ([\\/\\w\\.\\-=\\?]*)\\/?\\b    # match api or file or parameters\n",
    "    ''', re.X)\n",
    "    \n",
    "    return re.sub(url_exp2, 'urltoken', re.sub(url_exp1, 'urltoken', text))\n",
    "\n",
    "def replace_users(text):\n",
    "    return re.sub(r'(@\\w+)', 'usertagtoken', text)\n",
    "\n",
    "def replace_hashtags(text):\n",
    "    return re.sub(r'(#\\w+)', 'hashtagtoken', text)\n",
    "\n",
    "def replace_numbers(text):\n",
    "    price_exp = re.compile(r\"\\$(\\d*\\,){,}\\d+\\.?\\d*\")\n",
    "    pct_exp = re.compile(r'\\b(\\d*\\,){,}\\d+\\.?\\d*\\%')\n",
    "    counter_exp = re.compile(r\"(\\d*\\,){,}\\d+\\.?\\d*(st|nd|rd|th)s?\")\n",
    "    num_exp = re.compile(r\"\\b(\\d*\\,){,}\\d+\\.?\\d*\\b\")\n",
    "    text = re.sub(price_exp, 'moneytoken', text)\n",
    "    text = re.sub(pct_exp, 'percenttoken', text)\n",
    "    text = re.sub(counter_exp, 'countertoken', text)\n",
    "    return re.sub(num_exp, 'numbertoken', text)\n",
    "\n",
    "def read_corpus_basic(corp):\n",
    "    for doc in corp:\n",
    "        yield [x for x in gensim.utils.simple_preprocess(doc, deacc=True)]\n",
    "\n",
    "def read_corpus_with_stemming_and_SW_removal(corp):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for doc in corp:\n",
    "        yield [lemmatizer.lemmatize(x) for x in gensim.utils.simple_preprocess(doc, deacc=True)\n",
    "                   if x.lower() not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T17:18:54.241171",
     "start_time": "2018-12-01T17:18:54.153995"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded\n",
    "\n",
    "def isFileAvailable(filename):\n",
    "    try:\n",
    "        \n",
    "        with open(filename, 'r') as test:\n",
    "            print(\"File is ready!\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(\"The file is not ready at this moment!\" )\n",
    "        return False\n",
    "\n",
    "    \n",
    "def define_model_own():\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters = 32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    #model.summary()\n",
    "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "def classify(tokenizer,nn_model,text):\n",
    "        \n",
    "        test_post = []\n",
    "        test_post.append(text)\n",
    "        Xpredict = encode_docs(tokenizer,max_length, test_post)\n",
    "        i = nn_model.predict(Xpredict)\n",
    "        \n",
    "        if i[0][0]>0.5:\n",
    "            return (i[0][0],\"relevant\")\n",
    "        else:\n",
    "            return (i[0][0],\"irrlv\")\n",
    "\n",
    "\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "        score = analyser.polarity_scores(sentence)\n",
    "        return (score[\"compound\"])\n",
    "\n",
    "#Code to get json files from cluster\n",
    "def run_cmd(args_list):\n",
    "    print('Running system command: {0}'.format(' '.join(args_list)))\n",
    "    proc = subprocess.Popen(args_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    s_output, s_err = proc.communicate()\n",
    "    s_return =  proc.returncode\n",
    "    return s_return, s_output, s_err \n",
    "\n",
    "###Getting the Sentiment Label\n",
    "def sentiment_label(x):\n",
    "    \n",
    "    y = \"Neutral\"\n",
    "    if x < -0.2:\n",
    "        y = \"Negative\"\n",
    "    elif  x > 0.2:\n",
    "        y = \"Positive\"\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "    return (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.4/site-packages/keras/preprocessing/text.py:172: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 3s - loss: 0.4048 - acc: 0.8630\n",
      "Epoch 2/20\n",
      " - 3s - loss: 0.2602 - acc: 0.8880\n",
      "Epoch 3/20\n",
      " - 3s - loss: 0.2099 - acc: 0.8940\n",
      "Epoch 4/20\n",
      " - 3s - loss: 0.1910 - acc: 0.9088\n",
      "Epoch 5/20\n",
      " - 3s - loss: 0.1727 - acc: 0.9204\n",
      "Epoch 6/20\n",
      " - 3s - loss: 0.1561 - acc: 0.9306\n",
      "Epoch 7/20\n",
      " - 3s - loss: 0.1324 - acc: 0.9452\n",
      "Epoch 8/20\n",
      " - 3s - loss: 0.1263 - acc: 0.9479\n",
      "Epoch 9/20\n",
      " - 3s - loss: 0.1110 - acc: 0.9537\n",
      "Epoch 10/20\n",
      " - 3s - loss: 0.1038 - acc: 0.9589\n",
      "Epoch 11/20\n",
      " - 3s - loss: 0.0983 - acc: 0.9580\n",
      "Epoch 12/20\n",
      " - 3s - loss: 0.0900 - acc: 0.9637\n",
      "Epoch 13/20\n",
      " - 3s - loss: 0.0881 - acc: 0.9654\n",
      "Epoch 14/20\n",
      " - 3s - loss: 0.0811 - acc: 0.9690\n",
      "Epoch 15/20\n",
      " - 3s - loss: 0.0827 - acc: 0.9662\n",
      "Epoch 16/20\n",
      " - 3s - loss: 0.0782 - acc: 0.9684\n",
      "Epoch 17/20\n",
      " - 3s - loss: 0.0751 - acc: 0.9681\n",
      "Epoch 18/20\n",
      " - 3s - loss: 0.0709 - acc: 0.9697\n",
      "Epoch 19/20\n",
      " - 3s - loss: 0.0715 - acc: 0.9700\n",
      "Epoch 20/20\n",
      " - 3s - loss: 0.0704 - acc: 0.9670\n",
      "Train Accuracy: 97.171302\n",
      "Test Accuracy: 76.991150\n"
     ]
    }
   ],
   "source": [
    "#Data Prep before training the model\n",
    "dt = pd.read_csv('twitterlabel.csv')\n",
    "\n",
    "dt['Tokens'] = dt['Tokens'].astype(str).apply(lambda x: x.replace(\"^\", \" \"))\n",
    "\n",
    "dt['Label'] = dt['Label'].str.lower()\n",
    "dt['Label'] = dt['Label'].apply(lambda x: x.replace(\"e\", \"\")[0:5].strip())\n",
    "\n",
    "for i in range (0,len(dt['Label'])):\n",
    "        if dt['Label'][i]!='irrlv':\n",
    "            dt['Label'][i]='relevant'\n",
    "\n",
    "#Making csv file ready before re-training\n",
    "dt1 = dt[list(['Text','Tokens','Label'])]\n",
    "dt1.to_csv(\"newtraining.csv\",index = False)\n",
    "\n",
    "dt = pd.read_csv(\"newtraining.csv\")\n",
    "dt['Tokens'] = dt['Tokens'].astype(str)\n",
    "y = pd.get_dummies(dt[\"Label\"]).astype(float)\n",
    "\n",
    "data = dt\n",
    "train_size = int(len(data) * .8)\n",
    " \n",
    "train_posts = data['Tokens'][:train_size]\n",
    "train_tags = y[:train_size]\n",
    "train_files_names = data['Label'][:train_size]\n",
    " \n",
    "test_posts = data['Tokens'][train_size:]\n",
    "test_tags = y[train_size:]\n",
    "test_files_names = data['Label'][train_size:]\n",
    "train_docs = data['Tokens'][:train_size]\n",
    "\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded\n",
    "\n",
    "# 2 outputs\n",
    "num_labels = 2\n",
    "vocab_size = 20000\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "tokenizer2 = Tokenizer(nb_words=20000)\n",
    "tokenizer2.fit_on_texts(train_posts)\n",
    "sequences = tokenizer2.texts_to_sequences(train_posts)\n",
    "\n",
    "word_index = tokenizer2.word_index\n",
    "#print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('owntwitter.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "#print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "#Creating word embedding layer\n",
    "EMBEDDING_DIM = 1500\n",
    "MAX_SEQUENCE_LENGTH = max_length\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "def define_model_own():\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters = 32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    #model.summary()\n",
    "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "Xtrain = encode_docs(tokenizer2, max_length, train_docs)\n",
    "Xtest = encode_docs(tokenizer2, max_length, test_posts)\n",
    "\n",
    "# define model\n",
    "CNN_model_own = define_model_own()\n",
    "# fit network\n",
    "CNN_model_own.fit(Xtrain, train_tags, epochs=20, verbose=2)\n",
    "\n",
    "_, acc = CNN_model_own.evaluate(Xtrain, train_tags, verbose=0)\n",
    "print('Train Accuracy: %f' % (acc*100))\n",
    "# evaluate model on test dataset\n",
    "_, acc = CNN_model_own.evaluate(Xtest, test_tags, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Automated Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-02T14:38:46.631Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let the show begins!\n",
      "Latest available file in twitter_archive: tw20181208T0120.json\n",
      "Enough Data\n",
      "Done with storing training data!\n",
      "Done with Predictions!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/site-packages/ipykernel/__main__.py:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with Topic Modeling!\n",
      "Done with Sentiment Analyze\n",
      "Done with Sentiment Percentage Hourly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.4/site-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with Sentiment Percentage Daily\n",
      "Today is Saturday !\n",
      "Dont feel like training the model today!\n",
      "See you in 59.254659084479016 minutes\n",
      "Let the show begins!\n",
      "Latest available file in twitter_archive: tw20181208T0120.json\n",
      "New file not AVAILABLE! Waiting 10 min to check again.\n"
     ]
    }
   ],
   "source": [
    "file_hdfs_dir = '/data/atl_sprint_2018/twitter_archive/'\n",
    "file_name = ''\n",
    "local_file_name = 'local_twitter_data.json'\n",
    "\n",
    "while True:\n",
    "    print('Let the show begins!')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    count = 0\n",
    "    tweets_data = []\n",
    "    Time=[]\n",
    "    tweets=pd.DataFrame()\n",
    "    tweets_temp=pd.DataFrame()\n",
    "\n",
    "    #(ret, out, err)= run_cmd(['hadoop', 'fs', '-get', '/data/atl_sprint_2018/twitter_archive/tw20181115*'])\n",
    "\n",
    "\n",
    "    # Getting the file from cluster\n",
    "    tmp_file = file_name # this variable will store the name of the latest file in hdfs twitter_archive\n",
    "    while tmp_file==file_name:\n",
    "        read_status = os.system('hdfs dfs -ls {} | tail -1 >> hdfs_last_file.txt'.format(file_hdfs_dir))\n",
    "        with open('hdfs_last_file.txt','r') as lf:\n",
    "            tmp_file = os.path.basename(lf.read().split(' ')[-1].strip())\n",
    "            print('Latest available file in twitter_archive: '+tmp_file)\n",
    "        \n",
    "        # check if latest file in hdfs matches existing file_name\n",
    "        if tmp_file==file_name:\n",
    "            print(\"New file not AVAILABLE! Waiting 10 min to check again.\")\n",
    "            time.sleep(10*60)\n",
    "                \n",
    "    # update filename :-) and download file from hdfs (file is downloaded to local_file_name)\n",
    "    rm_status = os.system('rm {}'.format(local_file_name)) # remove local twitter file before overwriting :-)\n",
    "    file_name = tmp_file\n",
    "    get_status = os.system('hdfs dfs -get {} {}'.format(os.path.join(file_hdfs_dir, file_name), local_file_name))\n",
    "    # recover date, hour, and seconds (which for some reason are called \"i\")\n",
    "    file_time = parse(os.path.splitext(file_name)[0][-13:]) \n",
    "    #date1,hour, i = int(file_time.date().strftime('%Y%m%d')), file_time.hour, file_time.minute\n",
    "    date,hour, i = file_time.date(), file_time.hour, file_time.minute\n",
    "    date1 = datetime.datetime.strptime(str(file_time.date()),'%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "    date2 = datetime.datetime.strptime(str(file_time.date()),'%Y-%m-%d').strftime('%Y%m%-d')\n",
    "#     date1 = '2018-12-06'\n",
    "#     hour = 23\n",
    "\n",
    "    #Getting the weekday and hour\n",
    "    weekday = datetime.datetime.today().weekday()\n",
    "    train_hour = datetime.datetime.now().hour\n",
    "    day = datetime.datetime.now().strftime(\"%A\")\n",
    "\n",
    "\n",
    "######################################################################################################################    \n",
    "    # Reading the file in python\n",
    "    try:\n",
    "        with open(local_file_name, 'r') as f: \n",
    "            for line in f:\n",
    "                try:\n",
    "                    tweets_data.append(json.loads(line))\n",
    "                except:\n",
    "                    continue\n",
    "        count = count + len(tweets_data)\n",
    "\n",
    "        tweets_temp=pd.DataFrame()\n",
    "\n",
    "        tweets_temp['text'] = list(map(lambda tweet: tweet['text'], tweets_data))\n",
    "        tweets_temp['created_at'] = list(map(lambda tweet: tweet['created_at'], tweets_data))\n",
    "        tweets_temp['id_str'] = list(map(lambda tweet: tweet['id_str'], tweets_data))\n",
    "\n",
    "        tweets=tweets.append(tweets_temp).reset_index(drop=True)\n",
    "\n",
    "        #Removing Duplicates\n",
    "        unq_tweets=tweets.drop_duplicates(subset=['id_str','text','created_at'],keep='first',inplace=False).reset_index(drop=True)    \n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "\n",
    "    if len(unq_tweets) > 50:\n",
    "        print('Enough Data')\n",
    "\n",
    "    \n",
    "        \n",
    "        #Making Predictions using the trained model\n",
    "\n",
    "        df_predict = unq_tweets\n",
    "        df_predict['original_tweets'] = df_predict['text']\n",
    "        #Clean the csv file before making prediction\n",
    "        df_predict['text'] = df_predict.text.apply(remove_nonascii)\n",
    "        df_predict['text'] = df_predict.text.apply(replace_abbrevs)\n",
    "        df_predict['text'] = df_predict.text.apply(replace_email)\n",
    "        df_predict['text'] = df_predict.text.apply(replace_urls)\n",
    "        df_predict['text'] = df_predict.text.apply(replace_numbers)\n",
    "        df_predict['text'] = df_predict.text.apply(replace_hashtags)\n",
    "        df_predict['text'] = df_predict.text.apply(replace_users)\n",
    "        corp2 = list(read_corpus_with_stemming_and_SW_removal(df_predict.text))\n",
    "        bigrams2 = gensim.models.phrases.Phrases(threshold=50)\n",
    "        bigrams2.add_vocab(corp2)\n",
    "        bigram_phraser2 = gensim.models.phrases.Phraser(bigrams2)\n",
    "        df_predict['Tokens'] = [\"^\".join(bigram_phraser2[tokens]) for tokens in corp2]\n",
    "        df_predict['Tokens'] = df_predict['Tokens'].apply(lambda x: x.replace(\"^\", \" \"))\n",
    "        df_predict['Tokens'] = df_predict['Tokens'].apply(lambda x: x.replace(\"_\", \" \"))\n",
    "        df_predict[\"Tokens\"] = df_predict[\"Tokens\"].str.replace(\"usertagtoken\",\"\")\n",
    "        df_predict[\"Tokens\"] = df_predict[\"Tokens\"].str.replace(\"hashtagtoken\",\"\")\n",
    "        df_predict[\"Tokens\"] = df_predict[\"Tokens\"].str.replace(\"numbertoken\",\"\")\n",
    "        df_predict[\"Tokens\"] = df_predict[\"Tokens\"].str.replace(\"url\",\"\")\n",
    "        df_predict[\"Tokens\"] = df_predict[\"Tokens\"].str.replace(\"rt\",\"\")\n",
    "\n",
    "        #Only select the Tokens column for prediction\n",
    "    #     predict_docs = df_predict['Tokens']\n",
    "\n",
    "    #     Xpredict = encode_docs(tokenizer2, max_length, predict_docs)\n",
    "\n",
    "    #     CNN_model_own.predict(Xpredict) \n",
    "\n",
    "\n",
    "\n",
    "        classification,confidence=[],[]    \n",
    "        for t in df_predict['Tokens']:\n",
    "            conf,result=classify(tokenizer2, CNN_model_own,t)\n",
    "            classification.append(result)\n",
    "            confidence.append(conf)\n",
    "\n",
    "\n",
    "\n",
    "        df_predict['Label'] = pd.Series(classification)  #Add data to the dataframe\n",
    "        df_predict['Confidence'] = pd.Series(confidence) \n",
    "\n",
    "        df_newdata=df_predict[(df_predict['Confidence'] > 0.99) | (df_predict['Confidence'] <= 0.19)].reset_index(drop=True)\n",
    "        #Converting the predictions back to ire/relev\n",
    "        #y_predict = []  # array containing the binary classes output.\n",
    "\n",
    "        # Predicting the output (as interger array) from the x_test.\n",
    "    #     y_pred_new = CNN_model_own.predict(Xpredict)\n",
    "\n",
    "\n",
    "    #     for i in y_pred_new:\n",
    "    #         #print (i)\n",
    "    #         if i[0] > i[1]:\n",
    "    #             i[0] = int(1)\n",
    "    #             i[1] = int(0)\n",
    "    #             y_predict.append(\"irrlv\")\n",
    "    #         else:\n",
    "    #             i[0] = int(0)\n",
    "    #             i[1] = int(1)\n",
    "    #             y_predict.append(\"relevant\")\n",
    "\n",
    "\n",
    "        #Append the label column back to the dataframe\n",
    "        #df_predict['label'] = y_predict\n",
    "\n",
    "        #Export the dataframe to the csv file\n",
    "        #df_predict.to_csv('Twitter_Prediction.csv', index=False)\n",
    "\n",
    "\n",
    "        if os.path.exists(\"trainingdata.csv\"):\n",
    "            with open(\"trainingdata.csv\", 'a') as f:\n",
    "                df_newdata.to_csv(f, header=False,index=False)\n",
    "        else:\n",
    "            df_newdata.to_csv(\"trainingdata.csv\",index= False)\n",
    "\n",
    "        print(\"Done with storing training data!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        df_predict.to_csv('Twitter_Prediction.csv', index=False)\n",
    "        print(\"Done with Predictions!\")\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "        \n",
    "        #Topic Modeling\n",
    "        # Reading the csv file and selecting the relevant tweets rows.\n",
    "        data = pd.read_csv(\"Twitter_Prediction.csv\")\n",
    "        data_r = data['Label'] != \"irrlv\"\n",
    "        data_rel = data[data_r]\n",
    "        #data_rel\n",
    "\n",
    "        # Creating an array of the relevant tweet tokens.\n",
    "        doc_clean2 = []\n",
    "        data_rel['Tokens'] = data_rel[\"Tokens\"].astype(str)\n",
    "        for i in data_rel[\"Tokens\"]:\n",
    "            #print (i)\n",
    "\n",
    "            doc_clean2.append(i.split())\n",
    "\n",
    "\n",
    "\n",
    "        # Topic modelling using LDA\n",
    "\n",
    "        # Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
    "        dictionary = corpora.Dictionary(doc_clean2)\n",
    "\n",
    "        # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
    "        doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean2]\n",
    "\n",
    "        # Creating the object for LDA model using gensim library\n",
    "        Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "        # Running and Training LDA model on the document term matrix\n",
    "        ldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary, passes=50)\n",
    "\n",
    "\n",
    "\n",
    "        # Getting the Topic Modellng output in the required format.\n",
    "        pd_array = []\n",
    "\n",
    "\n",
    "\n",
    "        def get_lda_topics(model, num_topics):\n",
    "            word_dict = {};\n",
    "            for i in range(num_topics):\n",
    "                words = model.show_topic(i, topn = 10);\n",
    "                #print (i)\n",
    "                #print ([i[0] for i in words])\n",
    "                #print ([i[1]*1000 for i in words])\n",
    "                #print (\"--------\")\n",
    "\n",
    "\n",
    "                #print ([i,hour, date, [i[0] for i in words], [i[1]*1000 for i in words]])\n",
    "                #pd_array.append([i,hour, date, [i[0] for i in words], [i[1]*1000 for i in words]])\n",
    "\n",
    "\n",
    "                for j in words:\n",
    "                    #print ([i,hour, date,j[0], j[1]*1000])\n",
    "                    pd_array.append([i,hour, date1,j[0], j[1]*1000])\n",
    "\n",
    "        # Calling the function. This is important for appending in the list \"pd_array\".        \n",
    "        get_lda_topics(ldamodel, 5)    \n",
    "\n",
    "        # Creating the dataframe from the list.\n",
    "        df_topic_modelling = pd.DataFrame(pd_array, columns = [\"Topic Number\", \"Hour\", \"Date\", \"Token\", \"Frequency\"])\n",
    "\n",
    "\n",
    "\n",
    "        # This creates a new csv file and append to it\n",
    "\n",
    "\n",
    "        if os.path.exists(\"Twitter_Topic_Modelling_.csv\"):\n",
    "            with open(\"Twitter_Topic_Modelling_.csv\", 'a') as f:\n",
    "                df_topic_modelling.to_csv(f, header=False, sep = \"|\")\n",
    "        else:\n",
    "            df_topic_modelling.to_csv(\"Twitter_Topic_Modelling_.csv\", sep = \"|\")\n",
    "\n",
    "        print('Done with Topic Modeling!')\n",
    "\n",
    "######################################################################################################################\n",
    "        \n",
    "        \n",
    "        #Sentiment Analyze\n",
    "        # Vader Sentiment Analyser\n",
    "        analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Getting the list with the required data to convert to the dataframe.\n",
    "        sent_score = []\n",
    "\n",
    "        for i in data_rel[\"text\"]:\n",
    "            sent_score.append([i, date1, hour, sentiment_analyzer_scores(i)])\n",
    "\n",
    "\n",
    "        # Converting to the dataframe\n",
    "        df_twitter_sent = pd.DataFrame(sent_score, columns = [\"Tweet\", \"Date\", \"Hour\", \"Sentiment\"])\n",
    "\n",
    "        # Adding the original tweet as a column\n",
    "        data_rel[\"original_tweets\"]\n",
    "        df_twitter_sent[\"Original_Tweets\"] = data_rel[\"original_tweets\"].values\n",
    "\n",
    "\n",
    "\n",
    "        # This creates a new csv file and append to it\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if os.path.exists(\"Twitter_Sentiment.csv\"):\n",
    "            with open(\"Twitter_Sentiment.csv\", 'a') as f:\n",
    "                df_twitter_sent.to_csv(f, header=False, sep = \"|\")\n",
    "        else:\n",
    "            df_twitter_sent.to_csv(\"Twitter_Sentiment.csv\", sep = \"|\")\n",
    "\n",
    "        print(\"Done with Sentiment Analyze\")\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "        #Labeling the Sentiment Score\n",
    "        df_twitter_sent[\"Sentiment_Label\"] = df_twitter_sent[\"Sentiment\"].apply(sentiment_label)\n",
    "\n",
    "        ###Getting the label for certain hour\n",
    "        volume = df_twitter_sent[\"Sentiment_Label\"].value_counts()\n",
    "        volume = volume.to_dict()\n",
    "        sld = (df_twitter_sent.groupby(\"Sentiment_Label\").size()/df_twitter_sent[\"Sentiment_Label\"].count())*100\n",
    "        sldd = sld.to_dict()\n",
    "\n",
    "        df_volume = pd.DataFrame(list(volume.items()),columns=['Sentiment_Label', 'Volume'])\n",
    "        df_twitter_perc = pd.DataFrame(list(sldd.items()),columns=['Sentiment_Label', 'Sentiment_Percentage'])\n",
    "        df_twitter_perc[\"Hour\"] = hour\n",
    "        df_twitter_perc[\"Date\"] = date1\n",
    "        df_twitter_perc[\"Volume\"] = df_volume[\"Volume\"]\n",
    "        df_twitter_perc = df_twitter_perc[['Date','Hour','Sentiment_Label','Volume','Sentiment_Percentage']]\n",
    "\n",
    "\n",
    "\n",
    "        # This creates a new csv file\n",
    "\n",
    "\n",
    "\n",
    "        if os.path.exists(\"Twitter_Sentiment_Hourly.csv\"):\n",
    "            with open(\"Twitter_Sentiment_Hourly.csv\", 'a') as f:\n",
    "                df_twitter_perc.to_csv(f, header=False,index=False)\n",
    "        else:\n",
    "            df_twitter_perc.to_csv(\"Twitter_Sentiment_Hourly.csv\",index= False)\n",
    "\n",
    "        print(\"Done with Sentiment Percentage Hourly\")\n",
    "\n",
    "######################################################################################################################\n",
    "        \n",
    "        ###Getting daily sentiment\n",
    "        df_daily = pd.read_csv(\"Twitter_Sentiment_Hourly.csv\")\n",
    "        dh = df_daily[df_daily.Date != date2]\n",
    "        dh.to_csv(\"Twitter_Sentiment_Hourly.csv\",index= False)\n",
    "        daily1 = df_daily.loc[(df_daily['Date'] == date1)]\n",
    "        daily1.loc[daily1['Sentiment_Label'] == 'Positive', 'Volume'] = daily1.loc[daily1['Sentiment_Label'] == 'Positive', 'Volume'].sum()\n",
    "        daily1.loc[daily1['Sentiment_Label'] == 'Neutral', 'Volume'] = daily1.loc[daily1['Sentiment_Label'] == 'Neutral', 'Volume'].sum()\n",
    "        daily1.loc[daily1['Sentiment_Label'] == 'Negative', 'Volume'] = daily1.loc[daily1['Sentiment_Label'] == 'Negative', 'Volume'].sum()\n",
    "        daily1 = daily1.drop_duplicates(['Sentiment_Label'],keep='first')\n",
    "        daily1['Sentiment_Percentage'] = (daily1['Volume']/daily1['Volume'].sum())*100\n",
    "        daily1[\"Hour\"] = hour\n",
    "        daily1[\"Date\"] = date1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # This creates a new csv file\n",
    "        if os.path.exists(\"Twitter_Sentiment_Daily.csv\"):\n",
    "            daily = pd.read_csv('Twitter_Sentiment_Daily.csv')\n",
    "            frame = [daily, daily1]\n",
    "            new = pd.concat(frame)\n",
    "            new = new[new.Hour == 23]\n",
    "            #new = new.drop_duplicates(['Volume'],keep='first')\n",
    "            #print(\"Opps, its here!\")\n",
    "    \n",
    "            new.to_csv(\"Twitter_Sentiment_Daily.csv\",index = False)\n",
    "        else:\n",
    "            #print(\"Creating a new one now!\")\n",
    "            daily1.to_csv(\"Twitter_Sentiment_Daily.csv\",index = False)\n",
    "\n",
    "\n",
    "    #     # This appends the dataframe to an existing csv file.\n",
    "    #     daily = pd.read_csv('Twitter_Sentiment_Daily.csv')\n",
    "    #     frame = [daily, daily1]\n",
    "    #     new = pd.concat(frame)\n",
    "    #     new = new[new.Hour == 23]\n",
    "\n",
    "    #     with open(\"Twitter_Sentiment_Daily.csv\", 'a') as f:\n",
    "    #         print(\"Its here!\")\n",
    "    #         new.to_csv(f, header=False,index=False)\n",
    "\n",
    "\n",
    "        print(\"Done with Sentiment Percentage Daily\")\n",
    "        \n",
    "######################################################################################################################\n",
    "\n",
    "        #Trainning the CNN Model if its on Sunday 10am\n",
    "    \n",
    "        if weekday == 6 and train_hour == 10:\n",
    "            print('Its',day, 'today!')\n",
    "            print('Time to train the MODEL!')\n",
    "        \n",
    "        #Training the model now!\n",
    "            \n",
    "#             dt = pd.read_csv('twitterlabel.csv')\n",
    "\n",
    "#             dt['Tokens'] = dt['Tokens'].astype(str).apply(lambda x: x.replace(\"^\", \" \"))\n",
    "\n",
    "#             dt['Label'] = dt['Label'].str.lower()\n",
    "#             dt['Label'] = dt['Label'].apply(lambda x: x.replace(\"e\", \"\")[0:5].strip())\n",
    "\n",
    "#             for i in range (0,len(dt['Label'])):\n",
    "#                     if dt['Label'][i]!='irrlv':\n",
    "#                         dt['Label'][i]='relevant'\n",
    "\n",
    "#             dt1 = dt[list(['Text','Tokens','Label'])]\n",
    "#             dt1.to_csv(\"newtraining.csv\",index = False)\n",
    "            \n",
    "            \n",
    "            file_exists=False\n",
    "            file_exists = isFileAvailable('trainingdata.csv')\n",
    "            if file_exists:\n",
    "                tp = pd.read_csv(\"trainingdata.csv\")\n",
    "                tp.columns = ['Text', 'created_at', 'id_str','original_tweets', 'Tokens','Label','Confidence']\n",
    "                tp1 = tp[list(['Text','Tokens','Label'])]\n",
    "                with open(\"newtraining.csv\", 'a') as f:\n",
    "                    tp1.to_csv(f, header=False,index=False)\n",
    "                dt = pd.read_csv('newtraining.csv')\n",
    "                dt = dt.drop_duplicates(['Tokens'],keep='first')\n",
    "            else:\n",
    "                dt = pd.read_csv('newtraining.csv')\n",
    "\n",
    "\n",
    "\n",
    "            y = pd.get_dummies(dt[\"Label\"]).astype(float)\n",
    "            dt['Tokens'] = dt['Tokens'].astype(str)\n",
    "\n",
    "            data = dt\n",
    "            train_size = int(len(data) * .8)\n",
    "\n",
    "            train_posts = data['Tokens'][:train_size]\n",
    "            train_tags = y[:train_size]\n",
    "            train_files_names = data['Label'][:train_size]\n",
    "\n",
    "            test_posts = data['Tokens'][train_size:]\n",
    "            test_tags = y[train_size:]\n",
    "            test_files_names = data['Label'][train_size:]\n",
    "            train_docs = data['Tokens'][:train_size]\n",
    "\n",
    "            max_length = max([len(s.split()) for s in train_docs])\n",
    "\n",
    "\n",
    "\n",
    "            # 2 outputs\n",
    "            num_labels = 2\n",
    "            vocab_size = 20000\n",
    "            batch_size = 100\n",
    "\n",
    "\n",
    "            tokenizer2 = Tokenizer(nb_words=20000)\n",
    "            tokenizer2.fit_on_texts(train_posts)\n",
    "            sequences = tokenizer2.texts_to_sequences(train_posts)\n",
    "\n",
    "            word_index = tokenizer2.word_index\n",
    "            #print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "            embeddings_index = {}\n",
    "            f = open('owntwitter.txt')\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "            f.close()\n",
    "            #print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "            #Creating word embedding layer\n",
    "            EMBEDDING_DIM = 1500\n",
    "            MAX_SEQUENCE_LENGTH = max_length\n",
    "\n",
    "            embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "            for word, i in word_index.items():\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    # words not found in embedding index will be all-zeros.\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "\n",
    "            embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                        EMBEDDING_DIM,\n",
    "                                        weights=[embedding_matrix],\n",
    "                                        input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                        trainable=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            Xtrain = encode_docs(tokenizer2, max_length, train_docs)\n",
    "            Xtest = encode_docs(tokenizer2, max_length, test_posts)\n",
    "\n",
    "            # define model\n",
    "            CNN_model_own = define_model_own()\n",
    "            # fit network\n",
    "            CNN_model_own.fit(Xtrain, train_tags, epochs=20, verbose=2)\n",
    "\n",
    "            _, acc = CNN_model_own.evaluate(Xtrain, train_tags, verbose=0)\n",
    "            print('Train Accuracy: %f' % (acc*100))\n",
    "            # evaluate model on test dataset\n",
    "            _, acc = CNN_model_own.evaluate(Xtest, test_tags, verbose=0)\n",
    "            print('Test Accuracy: %f' % (acc*100))\n",
    "            \n",
    "            print('Done with TRAINING!')\n",
    "            print('See you Next Week!')\n",
    "        \n",
    "\n",
    "            runtime = time.time() - start_time\n",
    "\n",
    "            sleep_times = 3600 - runtime\n",
    "\n",
    "            print(\"See you in\", (sleep_times/60), \"minutes\")\n",
    "\n",
    "            \n",
    "            try:\n",
    "                time.sleep(sleep_times)\n",
    "            \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        else:\n",
    "            print('Today is',day,'!')\n",
    "            print('Dont feel like training the model today!')\n",
    "            \n",
    "            runtime = time.time() - start_time\n",
    "\n",
    "            sleep_times = 3600 - runtime\n",
    "\n",
    "            print(\"See you in\", (sleep_times/60), \"minutes\")\n",
    "            \n",
    "            \n",
    "            try:\n",
    "\n",
    "                time.sleep(sleep_times)\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "######################################################################################################################\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        print('Not Enough data!')\n",
    "        \n",
    "        daily = pd.read_csv('Twitter_Sentiment_Daily.csv')\n",
    "        frame = [daily, daily1]\n",
    "        new = pd.concat(frame)\n",
    "        new.to_csv(\"Twitter_Sentiment_Daily.csv\",index = False)\n",
    "        \n",
    "        print(\"Done with daily sentiment\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        runtime = time.time() - start_time\n",
    "\n",
    "        sleep_times = 3600 - runtime\n",
    "        \n",
    "        print('I will run it again', 'in', (sleep_times/60), 'minutes!')\n",
    "        \n",
    "        time.sleep(sleep_times)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
